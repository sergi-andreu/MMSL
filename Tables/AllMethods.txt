\begin{tabular}{llllll}
{} & {} & {Accuracy} & {Recall} & {Precision} & {F1 Score} \\
LDA & lsqr Solver & 77.5±5.2% & \font-weightbold 84.1±4.8% & 74.9±6.4% & 79.0±4.3% \\
QDA & lsqr Solver & 71.3±1.5% & \font-weightbold 72.2±5.3% & 70.8±3.7% & 71.3±3.2% \\
Logistic Regression & l1 Penalty & 76.7±5.7% & \font-weightbold 82.3±6.0% & 74.7±6.3% & 78.1±4.6% \\
\multirow[c]{2}{*}{SVM} & linear Kernel & 75.6±5.0% & \font-weightbold 80.2±4.6% & 73.5±6.2% & 76.6±4.9% \\
 & rbf Kernel & 80.2±5.3% & \font-weightbold 86.6±5.5% & 77.5±5.2% & 81.6±3.5% \\
\multirow[c]{2}{*}{K Nearest Neighbors} & 5 Number of neighbors & 75.7±4.8% & \font-weightbold 83.4±3.6% & 72.5±5.6% & 77.4±4.4% \\
 & 50 Number of neighbors & 73.9±4.5% & \font-weightbold 86.3±8.9% & 69.8±4.0% & 76.7±3.2% \\
\multirow[c]{2}{*}{Decision Trees} & gini Criterion & 75.9±4.5% & \font-weightbold 77.6±5.0% & 75.3±6.4% & 76.3±4.5% \\
 & entropy Criterion & 74.7±4.7% & \font-weightbold 75.0±6.0% & 75.0±3.5% & 74.8±3.5% \\
\multirow[c]{2}{*}{Random Forest} & 50 Number of estimators & 83.2±4.6% & \font-weightbold 87.3±4.2% & 81.3±7.8% & 83.9±4.3% \\
 & 150 Number of estimators & 83.8±4.3% & \font-weightbold 86.5±5.0% & 82.5±5.5% & 84.3±3.7% \\
\multirow[c]{2}{*}{Bagging} & 50 Number of estimators & 82.6±4.8% & \font-weightbold 84.3±6.1% & 82.4±7.1% & 83.0±4.1% \\
 & 150 Number of estimators & 82.6±3.4% & \font-weightbold 84.7±6.1% & 82.1±6.3% & 83.0±2.5% \\
\multirow[c]{2}{*}{NN} & (20, 10, 5) Hidden layers & 76.3±4.1% & 76.6±3.0% & \font-weightbold 76.7±5.8% & 76.5±3.1% \\
 & (5, 5) Hidden layers & 77.5±4.0% & \font-weightbold 80.7±4.5% & 76.3±5.4% & 78.3±3.2% \\
\end{tabular}
